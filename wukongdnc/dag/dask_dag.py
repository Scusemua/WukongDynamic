from distributed import LocalCluster, Client
import dask
import dask.array as da
from collections import defaultdict
from .DFS_visit import Node

def execute_task(task_obj: tuple, existing_results: dict = {}, task_id: str = None):
  """
  Execute the given task.
  
  The `task_obj` parameter is expected to be a tuple. 
    The first element of the tuple will be the function to be executed.
    The remaining elements are the inputs to that function.
  
  The `existing_results` is expected to be a dictionary.
    The keys are task IDs.
    The values are the outputs generated by those tasks.
  
  Returns the result of executing the task.
  
  Will store the output in the `existing_results` dictionary if the task key of the task to be executed is provided.
  """
  func_obj = task_obj[0]  # func object
  inputs = task_obj[1:]   # tuple of 0 or more values
  
  return execute_task(func_obj, inputs, existing_results = existing_results, task_id = task_id)

def execute_task(func_obj, inputs: tuple, existing_results: dict = {}, task_id: str = None):
  """
  Execute the given task.
  
  The `func_obj` parameter is expected to be a callable function object.
  
  The `inputs` parameter is a tuple of 0 or more inputs.
  
  The `existing_results` is expected to be a dictionary.
    The keys are task IDs.
    The values are the outputs generated by those tasks.
  
  Returns the result of executing the task.
  
  Will store the output in the `existing_results` dictionary if the task key of the task to be executed is provided.
  """  
  # If the inputs are strings, then they refer to the outputs of other tasks.
  # Replace the strings (which refer to Task IDs) with the result of the associated task.
  for i in range(0, len(inputs)):
    if type(inputs[i]) is str:
      input_task_id = inputs[i]
      
      # If we do not have any data for the input task, then raise an exception.
      if input_task_id not in existing_results:
        raise ValueError("We do not have any data for task %s" % input_task_id)
      
      # Replace the task ID with the output of the associated task.
      inputs[i] = existing_results[input_task_id]
  
  res = func_obj(*inputs)
  
  # If the ID of the task we just executed was provided, then store the result in the `existing_results` dictionary.
  if task_id is not None:
    existing_results[task_id] = res 
  
  return res

if __name__ == "__main__":
  lc = LocalCluster()
  c = Client(lc)
  s = lc.scheduler

  def add(x, y):
    return x + y

  def triple(x):
    return 3 * x

  def square(x):
    return x ** 2

  def multiply(x, y, z):
    return x * y * z

  def divide(x):
    quotient = x / 72
    print("quotient: " + str(quotient))
    return quotient

  def increment(x):
    return x + 1

  def manual_dag():
    inc0 = dask.delayed(increment)(0)
    inc1 = dask.delayed(increment)(1)
    trip = dask.delayed(triple)(inc1)
    sq = dask.delayed(square)(inc1)
    ad = dask.delayed(add)(inc1, inc0)
    mult = dask.delayed(multiply)(trip, sq, ad)
    div = dask.delayed(divide)(mult)

    graph = div.__dask_graph__()
    result = div.compute()

    return graph, result

  def tree_reduction(n = 32):
    """
      n (int):
        Size of the array on which to perform tree reduction.
      
      Returns
      -------
        Tuple where first element is the Dask HighLevelGraph object (or whatever the type is),
        and the second element is the result of running the tree reduction computation on Dask proper.
    """
    L = range(n) # 1,024 
    while len(L) > 1:
      L = list(map(dask.delayed(add), L[0::2], L[1::2]))
    
    graph = L[0].__dask_graph__()
    result = L[0].compute()

    return graph, result 
  
  def mat_mul(n = 10, c = 2):
    """
      n (int):
        We will multiply two n x n matrices.
      
      c (int):
        The size of the chunks into which the matrices will be partitioned
        when parallelizing the matrix multiplication operation.
      
      Returns
      -------
        Tuple where first element is the Dask HighLevelGraph object (or whatever the type is),
        and the second element is the result of running the tree reduction computation on Dask proper.
    """
    x = da.random.random((n, n), chunks = (c, c))
    y = da.random.random((n, n), chunks = (c, c))
    z = da.matmul(x, y)
    graph = z.__dask_graph__()
    result = z.compute() 

    return graph, result    
  
  # graph, result = manual_dag()
  # graph, result = tree_reduction(n = 32)
  graph, result = mat_mul(n = 10, c = 2)

  nodes = []                                                      
  nodes_map = {}
  dependencies = graph.dependencies
  dependents = defaultdict(list)

  # Compute the dependents (i.e., successors) of each node in the DAG.
  for task, deps in graph.dependencies.items():
    for dep in deps:
      dependents[dep].append(task)

  for task, layer in graph.layers.items():
    print("Task: %s" % task)
    print("Layer: %s" % str(layer))
    print("type(layer): %s\n" % str(type(layer)))
  
  print("\nProcessing layers now...\n")

  # For each of the DAG nodes, create a Node object.
  for task, layer in graph.layers.items():
    print("Processing task %s with layer type %s now..." % (task, str(type(layer))))
    if type(layer) is dask.highlevelgraph.MaterializedLayer:
      print("Processing MaterializedLayer now...")
      for task_key, task_obj in layer.mapping.items():
        print("Processing task: %s" % str(task_key))
        if task_key in dependencies:
          current_dependencies = list(dependencies[task_key])
        else:
          current_dependencies = []
        
        if task_key in dependents:
          current_dependents = dependents[task_key]
        else:
          current_dependents = []

        node = Node(pred = current_dependencies, succ = current_dependents, 
          task_name = task_key, task = task_obj[0], task_inputs = task_obj[1:]) #rhc
        nodes_map[task_key] = node 
        nodes.append(node)
    elif type(layer) is dask.blockwise.Blockwise:
      print("Processing Blockwise now...")
      node = Node(pred = list(dependencies[task]), succ = dependents[task], 
        task_name = task, task = layer.dsk[task][0], task_inputs = layer.dsk[task][1:]) #rhc
      nodes_map[task] = node 
      nodes.append(node)

      print("Processed task %s\n" % task)
    else:
      raise ValueError("Unknown layer type in HighLevelDask (i.e., the Dask graph has a layer in it, and we've not written code to process that type of layer). Layer type: " + str(type(layer)))

  for node in nodes:
    for i in range(0, len(node.pred)):
      node.pred[i] = nodes_map[node.pred[i]]
    for i in range(0, len(node.succ)):
      node.succ[i] = nodes_map[node.succ[i]]

  # Then for each node in the list, do node.generate_ops().
  #for node in nodes:
  #  node.generate_ops()

  # Then at the end, call Node.save_DAG_info(), which saves a 
  # pickle() file of stuff that was accumulated during DFS search.
  # Node.save_DAG_info()

  # Uncomment this line to generate an image showing the DAG.
  # dag_image = div.visualize("dag.png")

  #result = div.compute()

  print("Result of executing the workload:", result)

  # This prints a mapping from Task ID to the 'Task' object.
  # The 'Task' object is a n-tuple. 
  # The first element of the n-tuple is the function to be executed.
  # The remaining elements are the inputs.
  # For leaf tasks, the inputs will be the actual numerical values to be input to the function.
  # For non-leaf tasks, the inputs will be the IDs of other tasks (strings).

  task_objects_map = graph.layers         # Map from Task ID to the task objects.
  task_objects = []                       # List of all the task objects. 
  leaf_tasks_map = {}                     # Mapping from task ID to task object for leaf tasks.
  leaf_task_ids = []                      # List of task IDs of leaf tasks.
  leaf_tasks = []                         # List of task objects, where each task object is a leaf task.
  leaf_nodes = []

  # Compute the leaf tasks.
  for task_id, deps in graph.dependencies.items():
    if len(deps) == 0:
      leaf_task_ids.append(task_id)
      leaf_tasks.append(graph[task_id])
      leaf_tasks_map[task_id] = graph[task_id]
  
  # Grab the inputs for the leaf tasks and put them in the appropriate nodes.
  for leaf_task_id in leaf_task_ids:
    node = nodes_map[leaf_task_id]
    # rhc
    #node.set_task_inputs(task_objects_map[leaf_task_id][leaf_task_id][1:])
    leaf_nodes.append(node)

  print("")

  print("Leaf Task IDs:", str(leaf_task_ids))
  
  print("")
  
  print("Leaf Tasks:")
  for leaf_task_id, leaf_task in leaf_tasks_map.items():
    print("\t %s: %s" % (leaf_task_id, str(leaf_task)))
    
  print("")

  print("Mapping from Task ID to the 'Task' object")
  print("This shows task keys mapped to task objects.")
  print("The first element of the task object is the function to be executed.")
  print("The remaining elements are the input to the tasks.")
  print("The inputs may be task IDs. This specifies data dependencies between tasks.\n")
  for k,v in graph.layers.items():
    print("%s: %s" % (k,v[k]))
    task_objects.append(v[k])
  
  DFS_nodes = []

  def dfs(visited, node):  #function for dfs 
    task_name = node.get_task_name() 
    if task_name not in visited:
      print("DFS: visit task: " + task_name)
      DFS_nodes.append(node)
      visited.add(task_name)
      dependents = node.get_succ()
      for dependent_node in dependents:
        dependent_task_name = dependent_node.get_task_name() 
        print("neighbor_task_name: " + dependent_task_name)
        dfs(visited, nodes_map[dependent_task_name])

  visited = set() # Set to keep track of visited nodes of DAG.

  for leaf_node in leaf_nodes:
    dfs(visited, leaf_node)

  print()
  print("DFS_nodes:")
  for n in DFS_nodes:
    print(n.get_task_name())

  print()
  print("generate ops for DFS_nodes:")        
  for n in DFS_nodes:  
    print("generate_ops for: " + n.get_task_name())
    n.generate_ops()
      
  Node.save_DAG_info()  