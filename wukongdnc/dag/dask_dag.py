from distributed import LocalCluster, Client
import dask
import dask.array as da
from collections import defaultdict
from .DFS_visit import Node

# def execute_task(task_obj: tuple, existing_results: dict = {}, task_id: str = None):
#   """
#   Execute the given task.
  
#   The `task_obj` parameter is expected to be a tuple. 
#     The first element of the tuple will be the function to be executed.
#     The remaining elements are the inputs to that function.
  
#   The `existing_results` is expected to be a dictionary.
#     The keys are task IDs.
#     The values are the outputs generated by those tasks.
  
#   Returns the result of executing the task.
  
#   Will store the output in the `existing_results` dictionary if the task key of the task to be executed is provided.
#   """
#   func_obj = task_obj[0]  # func object
#   inputs = task_obj[1:]   # tuple of 0 or more values
  
#   return execute_task(func_obj, inputs, existing_results = existing_results, task_id = task_id)

def execute_task(func_obj, inputs: tuple, existing_results: dict = {}, task_id: str = None):
  """
  Execute the given task.
  
  The `func_obj` parameter is expected to be a callable function object.
  
  The `inputs` parameter is a tuple of 0 or more inputs.
  
  The `existing_results` is expected to be a dictionary.
    The keys are task IDs.
    The values are the outputs generated by those tasks.
  
  Returns the result of executing the task.
  
  Will store the output in the `existing_results` dictionary if the task key of the task to be executed is provided.
  """  
  # If the inputs are strings, then they refer to the outputs of other tasks.
  # Replace the strings (which refer to Task IDs) with the result of the associated task.
  for i in range(0, len(inputs)):
    if type(inputs[i]) is str:
      input_task_id = inputs[i]
      
      # If we do not have any data for the input task, then raise an exception.
      if input_task_id not in existing_results:
        raise ValueError("We do not have any data for task %s" % input_task_id)
      
      # Replace the task ID with the output of the associated task.
      inputs[i] = existing_results[input_task_id]
  
  res = func_obj(*inputs)
  
  # If the ID of the task we just executed was provided, then store the result in the `existing_results` dictionary.
  if task_id is not None:
    existing_results[task_id] = res 
  
  return res

if __name__ == "__main__":
  lc = LocalCluster()
  c = Client(lc)
  s = lc.scheduler

  def add(x, y):
    return x + y

  def twox_add(x, y):
    return 2*(x + y)

  def triple(x):
    return 3 * x

  def square(x):
    return x ** 2

  def multiply(x, y, z):
    return x * y * z

  def multiply_pair(x, y):
    return x * y

  def divide(x):
    quotient = x / 72
    print("quotient: " + str(quotient))
    return quotient

  def divide_by_18(x):
    quotient = x / 18
    print("quotient: " + str(quotient))
    return quotient

  def increment(x):
    return x + 1

  def half_of_product(x,y):
    # intend for x to be result of increment(1) = 2, so result is 2 / 2 = 1.0
    return (x * y) / 2

  def product(x,y):
    return x * y

  def manual_dag():
    print("==== GENERATING MANUALLY-CREATED DAG")
    inc0 = dask.delayed(increment)(0) # 1
    inc1 = dask.delayed(increment)(1) # 2
    trip = dask.delayed(triple)(inc1) # 6
    sq = dask.delayed(square)(inc1) # 4
    ad = dask.delayed(add)(inc1, inc0) # 1+2 = 3
    mult = dask.delayed(multiply)(trip, sq, ad)  # 6*4*3 = 72
    div = dask.delayed(divide)(mult) # 72 / 72 = 1.0

    graph = div.__dask_graph__()
    result = div.compute()

    return graph, result

  def manual_dag_test_batch_faninNBs():
    print("==== GENERATING MANUALLY-CREATED DAG for testing batching faninNBs")
    inc0 = dask.delayed(increment)(0) # 1
    inc1 = dask.delayed(increment)(1) # 2
    trip = dask.delayed(triple)(inc1) # 6
    sq = dask.delayed(square)(inc1) # 4
    #half = dask.delayed(halve)(inc1) # 1.0
    half_prod = dask.delayed(half_of_product)(inc1,sq) # (2 * 4) / 2 = 4
    ad = dask.delayed(add)(inc1, inc0) # 1+2 = 3
    mult = dask.delayed(multiply)(trip, half_prod, ad) #  6*4*3 = 72.0 (same as manual_dag)
    div2 = dask.delayed(divide)(mult) # = 72.0 / 72 = 1

    graph = div2.__dask_graph__()
    result = div2.compute()

    return graph, result

  def manual_dag_test_batch_two_faninNBs():
    print("==== GENERATING MANUALLY-CREATED DAG for testing batching faninNBs")
    inc0 = dask.delayed(increment)(0) # 1
    inc1 = dask.delayed(increment)(1) # 2
    twox_ad = dask.delayed(twox_add)(inc1, inc0) # 2(1+2) = 6   
    ad = dask.delayed(add)(inc1, inc0) # 1+2 = 3
    mult = dask.delayed(multiply_pair)(ad,twox_ad) #  3*6 = 18
    div2 = dask.delayed(divide_by_18)(mult) # = 18 / 18 = 1

    graph = div2.__dask_graph__()
    result = div2.compute()

    return graph, result

  def tree_reduction(n = 32):
    """
      n (int):
        Size of the array on which to perform tree reduction.
      
      Returns
      -------
        Tuple where first element is the Dask HighLevelGraph object (or whatever the type is),
        and the second element is the result of running the tree reduction computation on Dask proper.
    """
    print("==== GENERATING TREE REDUCTION (n = %d)" % n)

    L = range(n) # 1,024 
    while len(L) > 1:
      L = list(map(dask.delayed(add), L[0::2], L[1::2]))
    
    graph = L[0].__dask_graph__()
    result = L[0].compute()

    return graph, result 
  
  def mat_mul(n = 10, c = 2):
    """
      n (int):
        We will multiply two n x n matrices.
      
      c (int):
        The size of the chunks into which the matrices will be partitioned
        when parallelizing the matrix multiplication operation.
      
      Returns
      -------
        Tuple where first element is the Dask HighLevelGraph object (or whatever the type is),
        and the second element is the result of running the tree reduction computation on Dask proper.
    """
    print("==== GENERATING MATRIX MULTIPLICATION (n = %d, c = %d)" % (n, c))
    x = da.random.random((n, n), chunks = (c, c))
    y = da.random.random((n, n), chunks = (c, c))
    z = da.matmul(x, y)

    z.visualize("./matrix_multiplication_dag.png")

    graph = z.__dask_graph__()
    result = z.compute() 

    return graph, result    
  
  # graph, result = manual_dag()
  # graph, result = manual_dag_test_batch_faninNBs()
  graph, result = manual_dag_test_batch_two_faninNBs()
  # graph, result = tree_reduction(n = 32)
  # graph, result = mat_mul(n = 4, c = 2)

  graph_dict = graph.to_dict()

  print("There are %d individual tasks in this DAG." % len(graph_dict))

  # print("Graph Dictionary:")
  # for key, val in graph_dict.items():
    # print("%s: %s" % (key, val))
  # print()

  nodes = []                                                      
  nodes_map = {}
  dependencies = {}
  dependents = defaultdict(list)

  print("Dependencies:")
  # Compute the dependents (i.e., successors) of each node in the DAG.
  for task, deps in graph.get_all_dependencies().items():
    task = str(task)
    dependencies[task] = deps 
    for dep in deps:
      print("Task %s depends on task %s." % (task, dep))
      dep = str(dep)
      dependents[dep].append(task)
  print()

  # print("Graph Layers:")
  # for task, layer in graph.layers.items():
    # print("Task: %s" % task)
    # print("Layer: %s" % str(layer))
    # print("type(layer): %s\n" % str(type(layer)))
  # print()
  
  chunked_tasks = defaultdict(list)

  print("\nProcessing layers now...\n")
  task_objects_map = {}      # Map from Task ID to the task objects.

  # TODO(ben): iterate over tasks via graph.to_dict() rather than layer-by-layer.
  for task_key, task_obj in graph.to_dict().items():
    task_key = str(task_key)
    task_objects_map[task_key] = task_obj
    # print("Processing task %s now..." % str(task_key))
    if task_key in dependencies:
      current_dependencies = list(dependencies[task_key])
    else:
      current_dependencies = []
    
    if task_key in dependents:
      current_dependents = dependents[task_key]
    else:
      current_dependents = []
    
    print("Task %s has %d dependencies." % (task_key, len(current_dependencies)))
    print("Task %s has %d dependents." % (task_key, len(current_dependents)))

    node = Node(pred = current_dependencies, succ = current_dependents, 
      task_name = task_key, task = task_obj[0], task_inputs = task_obj[1:]) #rhc
    nodes_map[task_key] = node 
    nodes.append(node)

  # For each of the DAG nodes, create a Node object.
  # for task, layer in graph.layers.items():
  #   print("Processing task %s with layer type %s now..." % (task, str(type(layer))))
  #   if type(layer) is dask.highlevelgraph.MaterializedLayer:
  #     print("Processing MaterializedLayer now...")
  #     for task_key, task_obj in layer.mapping.items():

  #       if type(task_key) is tuple and task_key[0] == task:
  #         chunked_tasks[task].append(task_key)

  #       print("Processing task: %s" % str(task_key))
  #       if task_key in dependencies:
  #         current_dependencies = list(dependencies[task_key])
  #       else:
  #         current_dependencies = []
        
  #       if task_key in dependents:
  #         current_dependents = dependents[task_key]
  #       else:
  #         current_dependents = []

  #       node = Node(pred = current_dependencies, succ = current_dependents, 
  #         task_name = task_key, task = task_obj[0], task_inputs = task_obj[1:]) #rhc
  #       nodes_map[task_key] = node 
  #       nodes.append(node)
  #   elif type(layer) is dask.blockwise.Blockwise:
  #     print("Processing Blockwise now...")
  #     node = Node(pred = list(dependencies[task]), succ = dependents[task], 
  #       task_name = task, task = layer.dsk[task][0], task_inputs = layer.dsk[task][1:]) #rhc
  #     nodes_map[task] = node 
  #     nodes.append(node)

  #     print("Processed task %s\n" % task)
  #   else:
  #     raise ValueError("Unknown layer type in HighLevelDask (i.e., the Dask graph has a layer in it, and we've not written code to process that type of layer). Layer type: " + str(type(layer)))

  for node in nodes:
    for i in range(0, len(node.pred)):
      node.pred[i] = nodes_map[str(node.pred[i])]
    for i in range(0, len(node.succ)):
      node.succ[i] = nodes_map[str(node.succ[i])]

  # Then for each node in the list, do node.generate_ops().
  #for node in nodes:
  #  node.generate_ops()

  # Then at the end, call Node.save_DAG_info(), which saves a 
  # pickle() file of stuff that was accumulated during DFS search.
  # Node.save_DAG_info()

  # Uncomment this line to generate an image showing the DAG.
  # dag_image = div.visualize("dag.png")

  #result = div.compute()

  print("Result of executing the workload:", result)

  # This prints a mapping from Task ID to the 'Task' object.
  # The 'Task' object is a n-tuple. 
  # The first element of the n-tuple is the function to be executed.
  # The remaining elements are the inputs.
  # For leaf tasks, the inputs will be the actual numerical values to be input to the function.
  # For non-leaf tasks, the inputs will be the IDs of other tasks (strings).

  task_objects = []                       # List of all the task objects. 
  leaf_tasks_map = {}                     # Mapping from task ID to task object for leaf tasks.
  leaf_task_ids = []                      # List of task IDs of leaf tasks.
  leaf_tasks = []                         # List of task objects, where each task object is a leaf task.
  leaf_nodes = []

  # Compute the leaf tasks.
  for task_id, deps in graph.get_all_dependencies().items():
    task_id = str(task_id)
    # print("Checking if task %s is a leaf task now..." % task_id)
    if len(deps) == 0:
      leaf_task_ids.append(task_id)
      leaf_tasks.append(task_objects_map[task_id])
      leaf_tasks_map[task_id] = task_objects_map[task_id]
  
  # Grab the inputs for the leaf tasks and put them in the appropriate nodes.
  for leaf_task_id in leaf_task_ids:
    node = nodes_map[leaf_task_id]
    # rhc
    #node.set_task_inputs(task_objects_map[leaf_task_id][leaf_task_id][1:])
    leaf_nodes.append(node)

  print("")

  print("Leaf Task IDs:", str(leaf_task_ids))
  
  print("")
  
  print("Leaf Tasks:")
  for leaf_task_id, leaf_task in leaf_tasks_map.items():
    print("\t %s: %s" % (leaf_task_id, str(leaf_task)))
    
  print("")

  print("Mapping from Task ID to the 'Task' object")
  print("This shows task keys mapped to task objects.")
  print("The first element of the task object is the function to be executed.")
  print("The remaining elements are the input to the tasks.")
  print("The inputs may be task IDs. This specifies data dependencies between tasks.\n")
  for k,v in graph.to_dict().items():
    task_objects.append(v)
  
  DFS_nodes = []

  def dfs(visited, node):  #function for dfs 
    task_name = node.get_task_name() 
    if task_name not in visited:
      print("DFS: visit task: " + str(task_name))
      DFS_nodes.append(node)
      visited.add(str(task_name))
      dependents = node.get_succ()
      for dependent_node in dependents:
        dependent_task_name = dependent_node.get_task_name() 
        print("neighbor_task_name: " + dependent_task_name)
        dfs(visited, nodes_map[dependent_task_name])

  visited = set() # Set to keep track of visited nodes of DAG.

  for leaf_node in leaf_nodes:
    dfs(visited, leaf_node)

  print()
  print("DFS_nodes:")
  for n in DFS_nodes:
    print(n.get_task_name())

  print()
  print("generate ops for DFS_nodes:")        
  for n in DFS_nodes:  
    print("generate_ops for: " + n.get_task_name())
    n.generate_ops()
      
  Node.save_DAG_info() 


"""
manual_dag:
DAG_map:
1  :    task: increment-ae88130b-bc16-45fb-8479-eb35fae7f83a, fanouts:['triple-a554a391-a774-4e71-91f9-a1c8e828a454', 'square-ea198bd4-0fb7-425b-8b83-acdad7d09028'],fanins:[],faninsNB:['add-a35dba4d-ff44-4853-a814-a7804da54c11'],collapse:[]fanin_sizes:[],faninNB_sizes:[2]task_inputs: (1,)
2  :    task: add-a35dba4d-ff44-4853-a814-a7804da54c11, fanouts:[],fanins:['multiply-137dd808-d202-4b59-9f38-2cdb5bf0985e'],faninsNB:[],collapse:[]fanin_sizes:[3],faninNB_sizes:[]task_inputs: ('increment-ae88130b-bc16-45fb-8479-eb35fae7f83a', 'increment-b2c04dbb-da27-4bce-aac2-c01d9d69c52d')
3  :    task: triple-a554a391-a774-4e71-91f9-a1c8e828a454, fanouts:[],fanins:['multiply-137dd808-d202-4b59-9f38-2cdb5bf0985e'],faninsNB:[],collapse:[]fanin_sizes:[3],faninNB_sizes:[]task_inputs: ('increment-ae88130b-bc16-45fb-8479-eb35fae7f83a',)
4  :    task: square-ea198bd4-0fb7-425b-8b83-acdad7d09028, fanouts:[],fanins:['multiply-137dd808-d202-4b59-9f38-2cdb5bf0985e'],faninsNB:[],collapse:[]fanin_sizes:[3],faninNB_sizes:[]task_inputs: ('increment-ae88130b-bc16-45fb-8479-eb35fae7f83a',)
5  :    task: multiply-137dd808-d202-4b59-9f38-2cdb5bf0985e, fanouts:[],fanins:[],faninsNB:[],collapse:['divide-4311366c-ac97-4af9-a04c-1a76a586b2ad']fanin_sizes:[],faninNB_sizes:[]task_inputs: ('triple-a554a391-a774-4e71-91f9-a1c8e828a454', 'square-ea198bd4-0fb7-425b-8b83-acdad7d09028', 'add-a35dba4d-ff44-4853-a814-a7804da54c11')
6  :    task: divide-4311366c-ac97-4af9-a04c-1a76a586b2ad, fanouts:[],fanins:[],faninsNB:[],collapse:[]fanin_sizes:[],faninNB_sizes:[]task_inputs: ('multiply-137dd808-d202-4b59-9f38-2cdb5bf0985e',)
7  :    task: increment-b2c04dbb-da27-4bce-aac2-c01d9d69c52d, fanouts:[],fanins:[],faninsNB:['add-a35dba4d-ff44-4853-a814-a7804da54c11'],collapse:[]fanin_sizes:[],faninNB_sizes:[2]task_inputs: (0,)
"
"""

"""
manual_dag_test_batch_faninNBs:
  DAG_map:
1  :   task: increment-95886556-3506-41d2-8e3c-4bc940ed0336, fanouts:['triple-94cc8328-63c6-40a8-b0df-3a8188fe5284', 'square-549ef5b2-b2cc-43f4-b771-61a650d7adc4'],fanins:[],faninsNB:['half_of_product-d5e48aca-729f-4991-97c3-24d32b105cc7', 'add-6321d36e-6f23-4ba9-b2ba-a4f1308ba286'],collapse:[]fanin_sizes:[],faninNB_sizes:[2, 2]task_inputs: (1,)
2  :   task: half_of_product-d5e48aca-729f-4991-97c3-24d32b105cc7, fanouts:[],fanins:['multiply-c2acd53c-2cb0-4aa7-bfcf-9a00e89dc894'],faninsNB:[],collapse:[]fanin_sizes:[3],faninNB_sizes:[]task_inputs: ('increment-95886556-3506-41d2-8e3c-4bc940ed0336', 'square-549ef5b2-b2cc-43f4-b771-61a650d7adc4')
3  :   task: add-6321d36e-6f23-4ba9-b2ba-a4f1308ba286, fanouts:[],fanins:['multiply-c2acd53c-2cb0-4aa7-bfcf-9a00e89dc894'],faninsNB:[],collapse:[]fanin_sizes:[3],faninNB_sizes:[]task_inputs: ('increment-95886556-3506-41d2-8e3c-4bc940ed0336', 'increment-630d25d0-17a8-4d7d-8ac0-2859814b1f4a')
4  :   task: triple-94cc8328-63c6-40a8-b0df-3a8188fe5284, fanouts:[],fanins:['multiply-c2acd53c-2cb0-4aa7-bfcf-9a00e89dc894'],faninsNB:[],collapse:[]fanin_sizes:[3],faninNB_sizes:[]task_inputs: ('increment-95886556-3506-41d2-8e3c-4bc940ed0336',)
5  :   task: square-549ef5b2-b2cc-43f4-b771-61a650d7adc4, fanouts:[],fanins:[],faninsNB:['half_of_product-d5e48aca-729f-4991-97c3-24d32b105cc7'],collapse:[]fanin_sizes:[],faninNB_sizes:[2]task_inputs: ('increment-95886556-3506-41d2-8e3c-4bc940ed0336',)
6  :   task: multiply-c2acd53c-2cb0-4aa7-bfcf-9a00e89dc894, fanouts:[],fanins:[],faninsNB:[],collapse:['divide-dd5039d9-fd56-4e12-a939-6cf5835fedf2']fanin_sizes:[],faninNB_sizes:[]task_inputs: ('triple-94cc8328-63c6-40a8-b0df-3a8188fe5284', 'half_of_product-d5e48aca-729f-4991-97c3-24d32b105cc7', 'add-6321d36e-6f23-4ba9-b2ba-a4f1308ba286')
7  :   task: divide-dd5039d9-fd56-4e12-a939-6cf5835fedf2, fanouts:[],fanins:[],faninsNB:[],collapse:[]fanin_sizes:[],faninNB_sizes:[]task_inputs: ('multiply-c2acd53c-2cb0-4aa7-bfcf-9a00e89dc894',)
8  :   task: increment-630d25d0-17a8-4d7d-8ac0-2859814b1f4a, fanouts:[],fanins:[],faninsNB:['add-6321d36e-6f23-4ba9-b2ba-a4f1308ba286'],collapse:[]fanin_sizes:[],faninNB_sizes:[2]task_inputs: (0,)
"""

"""
manual_dag_test_batch_two_faninNBs:
DAG_map:
1  :   task: increment-ef3cb2d1-7d74-470e-b956-a165590f2bad, fanouts:[],fanins:[],faninsNB:['add-576b91ef-5e17-4e00-b106-24bc8b535816', 'twox_add-09d703bb-55d1-45ad-90f2-aaf774682b91'],collapse:[]fanin_sizes:[],faninNB_sizes:[2, 2]task_inputs: (1,)
2  :   task: add-576b91ef-5e17-4e00-b106-24bc8b535816, fanouts:[],fanins:['multiply_pair-7000d437-7f74-4957-9d9e-4f618191616c'],faninsNB:[],collapse:[]fanin_sizes:[2],faninNB_sizes:[]task_inputs: ('increment-ef3cb2d1-7d74-470e-b956-a165590f2bad', 'increment-06e9bfdd-0e08-4b46-8237-bf353ef90c53')
3  :   task: twox_add-09d703bb-55d1-45ad-90f2-aaf774682b91, fanouts:[],fanins:['multiply_pair-7000d437-7f74-4957-9d9e-4f618191616c'],faninsNB:[],collapse:[]fanin_sizes:[2],faninNB_sizes:[]task_inputs: ('increment-ef3cb2d1-7d74-470e-b956-a165590f2bad', 'increment-06e9bfdd-0e08-4b46-8237-bf353ef90c53')
4  :   task: multiply_pair-7000d437-7f74-4957-9d9e-4f618191616c, fanouts:[],fanins:[],faninsNB:[],collapse:['divide_by_18-e7257418-f1bc-4b0c-b9b6-12e3197875ee']fanin_sizes:[],faninNB_sizes:[]task_inputs: ('add-576b91ef-5e17-4e00-b106-24bc8b535816', 'twox_add-09d703bb-55d1-45ad-90f2-aaf774682b91')
5  :   task: divide_by_18-e7257418-f1bc-4b0c-b9b6-12e3197875ee, fanouts:[],fanins:[],faninsNB:[],collapse:[]fanin_sizes:[],faninNB_sizes:[]task_inputs: ('multiply_pair-7000d437-7f74-4957-9d9e-4f618191616c',)
6  :   task: increment-06e9bfdd-0e08-4b46-8237-bf353ef90c53, fanouts:[],fanins:[],faninsNB:['add-576b91ef-5e17-4e00-b106-24bc8b535816', 'twox_add-09d703bb-55d1-45ad-90f2-aaf774682b91'],collapse:[]fanin_sizes:[],faninNB_sizes:[2, 2]task_inputs: (0,)

"""